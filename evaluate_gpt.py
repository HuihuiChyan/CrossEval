import json
import argparse
import random
import time
import json
import requests
import multiprocessing
from functools import partial

from evaluate_judge import build_dataset, calculate_metrics


def build_params_gpt():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model-type",
        type=str,
        choices=("gpt-4", "gpt-3.5"),
        default=None,
    )
    parser.add_argument(
        "--prompt-type",
        type=str,
        choices=("vanilla", "cot"),
        default=None,
    )
    parser.add_argument(
        "--data-type",
        type=str,
        choices=("judgelm", "pandalm", "auto-j", "prometheus-ind", "prometheus-ood",
                 "llmbar-neighbor", "llmbar-natural", "llmbar-gptinst", "llmbar-gptout", "llmbar-manual"),
        default=None,
    )
    parser.add_argument(
        "--data-path",
        type=str,
        default="./data",
    )
    parser.add_argument(
        "--max-new-token",
        type=int,
        default=2048,
        help="The maximum number of new tokens.",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="The temperature for sampling.",
    )
    # parser.add_argument(
    #     "--top-p",
    #     type=float,
    #     default=1.0,
    #     help="The temperature for sampling.",
    # )
    parser.add_argument(
        "--logit-file",
        type=str,
        default=None
    )
    parser.add_argument(
        "--pool-number",
        type=int,
        default=8,
    )
    parser.add_argument(
        "--multi-process",
        type=str,
        default="False",
    )
    args = parser.parse_args()
    return args


def request_gpt(prompt, model, temperature, max_new_token):

    if model == "gpt-3.5":
        model = "gpt-3.5-turbo"
    elif model == "gpt-4":
        model = "gpt-4-1106-preview"
    url = "https://api.chatgpt-3.vip/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-YbA0PBLo6X76clo88aAb29Fc0852428c8850390375AbA32d",
    }
    max_tries = 5
    res = ''
    response = None
    for i in range(max_tries):
        try:
            messages = [{"role": "user", "content": prompt}]
            data = {"model": model, "messages": messages,
                    "temperature": temperature, "max_tokens": max_new_token}
            response = requests.post(
                url, headers=headers, data=json.dumps(data))
            response = response.json()
            res = response['choices'][0]['message']['content'].strip()
            break
        except Exception as e:
            print("Exception! The response is " + str(response))
            time.sleep(5)
            continue
    return res


def parse_score_gpt(review, prompt_type, is_pair=True):
    if is_pair:
        if prompt_type == "cot":
            evaluation = review.strip()

            pos = evaluation.rfind('Therefore, ')
            label = [-1, -1]

            if pos != -1:
                decision = evaluation[pos +
                                      len('Therefore, '):].strip().lower()

                if decision.startswith('output (a) is better.'):
                    return [1, 0]
                elif decision.startswith('output (b) is better.'):
                    return [0, 1]
                elif decision.startswith('there is a tie.'):
                    return [1, 1]

            return label
        else:
            if review == "Output (a)":
                return [1, 0]
            elif review == "Output (b)":
                return [0, 1]
            elif review == "Tie":
                return [1, 1]
            else:
                return [1, 1]  # default is Tie
    else:
        try:
            score = review.split('\n')[0].strip()
            return float(score)
        except Exception as e:
            return 5.0  # default is middle score


def create_prompt_gpt(data_type, prompt_type):
    if prompt_type == "vanilla":
        if "prometheus" not in data_type:
            instruction = """<|im_start|>system
You are a helpful assistant tasked with evaluating the outputs from two different AI chatbots based on a given instruction. Your objective is to identify which output adheres better to the instruction or if they are of equal quality.
<|im_end|>
<|im_start|>user
Evaluate and compare Output (a) and Output (b) which are generated by two different AI chatbots. Determine if either output demonstrates a clearer understanding of the instruction or if both display a comparable level of competency.

Do NOT provide any explanation for your choice.
You should answer using ONLY "Output (a)" or "Output (b)" or "Tie". Do NOT output any other words.

# Instruction:
{question_body}

# Output (a):
{answer1_body}

# Output (b):
{answer2_body}

# Which is better, Output (a), Output (b), or Tie? Your response should be "Output (a)", "Output (b)", or "Tie":
<|im_end|>:"""

        else:
            # We use Prometheus prompt directly.
            instruction = """You are a fair evaluator language model.

###Task Description:
An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"
4. Please do not generate any other opening, closing, and explanations.

###The instruction to evaluate:
{question_body}

###Response to evaluate:
{answer_body}

###Score Rubrics:
{rubric}

###Feedback: [/INST]"""

    elif prompt_type == "cot":
        if "prometheus" not in data_type:
            instruction = """<|im_start|>system
You are a helpful assistant tasked with evaluating the outputs from two different AI chatbots based on a given instruction. Your objective is to identify which output adheres better to the instruction or if they are of equal quality.
<|im_end|>
<|im_start|>user
Evaluate and compare Output (a) and Output (b) from two different AI chatbots by considering the instruction given. Use your reasoning to assess which output more accurately and completely responds to the instruction.

Provide your assessment with a brief explanation, and conclude your response with one of the following statements:
- "Therefore, Output (a) is better."
- "Therefore, Output (b) is better."
- "Therefore, there is a tie."

Your explanation should directly precede the evaluative statement and should not include extraneous commentary.

# Instruction:
{question_body}

# Output (a):
{answer1_body}

# Output (b):
{answer2_body}

# Evaluation (Explain your reasoning concisely and end with one of the specified evaluative statements):
<|im_end|>:"""

    return instruction


def gpt_scoring(prompt, model):

    prediction = request_gpt(
        prompt, model, temperature=0.0, max_new_token=1024)

    counter.value += 1
    print(f"gpt_scoring {counter.value} finished.")

    return prediction


def init(c):
    global counter
    counter = c


if __name__ == "__main__":
    args = build_params_gpt()
    random.seed(42)

    dataset = build_dataset(args.data_type, args.data_path)

    instruction = create_prompt_gpt(args.data_type, args.prompt_type)

    prompts = []
    answers = []
    for example in dataset:
        if "prometheus" in args.data_type:
            prompt = instruction.format(question_body=example["question_body"],
                                        rubric=example["rubric"],
                                        answer_body=example["answer_body"])
            prompts.append(prompt)
        else:
            example["rubric"] = "Please rate the helpfulness, relevance, accuracy, level of details of their responses."
            prompt = instruction.format(question_body=example["question_body"],
                                        rubric=example["rubric"],
                                        answer1_body=example["answer1_body"],
                                        answer2_body=example["answer2_body"])
            prompts.append(prompt)

        answers.append(example["score"])

    manager = multiprocessing.Manager()
    counter = manager.Value("counter", 0)
    pool = multiprocessing.Pool(
        processes=args.pool_number, initializer=init, initargs=(counter,))

    if args.multi_process == "False":
        predictions = [gpt_scoring(sample, args.model_type)
                       for sample in prompts]
    else:
        pool_fn = partial(gpt_scoring, model=args.model_type)
        predictions = pool.map(pool_fn, prompts)

    pred_scores = [parse_score_gpt(p, args.prompt_type) for p in predictions]

    if args.logit_file is not None:
        with open(args.logit_file, "w", encoding="utf-8") as fout:
            for pred in pred_scores:
                fout.write(json.dumps(pred)+"\n")

    import pdb
    pdb.set_trace()
    print(args)
    metrics_dicts = calculate_metrics(answers, pred_scores, args.data_type)
    print(f"model: {args.model_type}, data: {args.data_type}")
    print(metrics_dicts)
