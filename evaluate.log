INFO 01-11 16:44:56 llm_engine.py:70] Initializing an LLM engine with config: model='models/autoj-13b', tokenizer='models/autoj-13b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 01-11 16:45:12 llm_engine.py:275] # GPU blocks: 3715, # CPU blocks: 327
INFO 01-11 16:45:14 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-11 16:45:14 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 01-11 16:45:17 model_runner.py:547] Graph capturing finished in 3 secs.
model loaded
Token indices sequence length is longer than the specified maximum sequence length for this model (4453 > 4096). Running this sequence through the model will result in indexing errors
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s]Processed prompts:   1%|          | 1/100 [00:17<28:40, 17.38s/it]Processed prompts:   2%|▏         | 2/100 [00:17<11:53,  7.28s/it]Processed prompts:   3%|▎         | 3/100 [00:18<07:18,  4.52s/it]Processed prompts:   4%|▍         | 4/100 [00:19<04:33,  2.85s/it]Processed prompts:   5%|▌         | 5/100 [00:21<04:07,  2.61s/it]Processed prompts:   6%|▌         | 6/100 [00:21<03:03,  1.95s/it]Processed prompts:   7%|▋         | 7/100 [00:22<02:14,  1.45s/it]Processed prompts:   8%|▊         | 8/100 [00:22<01:36,  1.05s/it]Processed prompts:   9%|▉         | 9/100 [00:23<01:31,  1.00s/it]Processed prompts:  10%|█         | 10/100 [00:24<01:27,  1.03it/s]Processed prompts:  11%|█         | 11/100 [00:24<01:06,  1.33it/s]Processed prompts:  13%|█▎        | 13/100 [00:24<00:42,  2.06it/s]Processed prompts:  15%|█▌        | 15/100 [00:25<00:38,  2.21it/s]Processed prompts:  16%|█▌        | 16/100 [00:26<00:38,  2.19it/s]Processed prompts:  17%|█▋        | 17/100 [00:26<00:40,  2.04it/s]Processed prompts:  19%|█▉        | 19/100 [00:27<00:36,  2.23it/s]Processed prompts:  20%|██        | 20/100 [00:27<00:30,  2.58it/s]Processed prompts:  21%|██        | 21/100 [00:27<00:25,  3.15it/s]Processed prompts:  22%|██▏       | 22/100 [00:28<00:26,  2.94it/s]Processed prompts:  23%|██▎       | 23/100 [00:28<00:29,  2.64it/s]Processed prompts:  24%|██▍       | 24/100 [00:29<00:29,  2.59it/s]Processed prompts:  25%|██▌       | 25/100 [00:29<00:25,  2.99it/s]Processed prompts:  26%|██▌       | 26/100 [00:29<00:26,  2.76it/s]Processed prompts:  27%|██▋       | 27/100 [00:29<00:20,  3.50it/s]Processed prompts:  28%|██▊       | 28/100 [00:30<00:26,  2.75it/s]Processed prompts:  29%|██▉       | 29/100 [00:31<00:32,  2.18it/s]Processed prompts:  30%|███       | 30/100 [00:32<00:40,  1.72it/s]Processed prompts:  31%|███       | 31/100 [00:32<00:38,  1.80it/s]Processed prompts:  32%|███▏      | 32/100 [00:32<00:29,  2.28it/s]Processed prompts:  33%|███▎      | 33/100 [00:33<00:27,  2.43it/s]Processed prompts:  34%|███▍      | 34/100 [00:33<00:37,  1.78it/s]Processed prompts:  35%|███▌      | 35/100 [00:34<00:39,  1.65it/s]Processed prompts:  36%|███▌      | 36/100 [00:35<00:33,  1.90it/s]Processed prompts:  37%|███▋      | 37/100 [00:35<00:28,  2.25it/s]Processed prompts:  38%|███▊      | 38/100 [00:35<00:24,  2.55it/s]Processed prompts:  40%|████      | 40/100 [00:36<00:26,  2.25it/s]Processed prompts:  41%|████      | 41/100 [00:36<00:25,  2.35it/s]Processed prompts:  42%|████▏     | 42/100 [00:37<00:31,  1.81it/s]Processed prompts:  43%|████▎     | 43/100 [00:38<00:30,  1.84it/s]Processed prompts:  44%|████▍     | 44/100 [00:38<00:29,  1.90it/s]Processed prompts:  45%|████▌     | 45/100 [00:39<00:34,  1.62it/s]Processed prompts:  46%|████▌     | 46/100 [00:40<00:34,  1.57it/s]Processed prompts:  54%|█████▍    | 54/100 [00:42<00:17,  2.61it/s]Processed prompts:  55%|█████▌    | 55/100 [00:42<00:15,  2.81it/s]Processed prompts:  56%|█████▌    | 56/100 [00:44<00:25,  1.72it/s]Processed prompts:  57%|█████▋    | 57/100 [00:45<00:22,  1.92it/s]Processed prompts:  58%|█████▊    | 58/100 [00:46<00:33,  1.27it/s]Processed prompts:  59%|█████▉    | 59/100 [00:47<00:32,  1.26it/s]Processed prompts:  60%|██████    | 60/100 [00:48<00:28,  1.41it/s]Processed prompts:  62%|██████▏   | 62/100 [00:48<00:17,  2.14it/s]Processed prompts:  63%|██████▎   | 63/100 [00:48<00:15,  2.36it/s]Processed prompts:  64%|██████▍   | 64/100 [00:48<00:13,  2.67it/s]Processed prompts:  65%|██████▌   | 65/100 [00:50<00:26,  1.33it/s]Processed prompts:  66%|██████▌   | 66/100 [00:51<00:22,  1.54it/s]Processed prompts:  67%|██████▋   | 67/100 [00:51<00:18,  1.80it/s]Processed prompts:  70%|███████   | 70/100 [00:51<00:09,  3.02it/s]Processed prompts:  71%|███████   | 71/100 [00:52<00:08,  3.46it/s]Processed prompts:  73%|███████▎  | 73/100 [00:53<00:11,  2.28it/s]Processed prompts:  74%|███████▍  | 74/100 [00:53<00:10,  2.41it/s]Processed prompts:  75%|███████▌  | 75/100 [00:54<00:11,  2.19it/s]Processed prompts:  76%|███████▌  | 76/100 [00:55<00:12,  1.88it/s]Processed prompts:  77%|███████▋  | 77/100 [00:55<00:11,  2.02it/s]Processed prompts:  79%|███████▉  | 79/100 [00:55<00:07,  2.76it/s]Processed prompts:  81%|████████  | 81/100 [00:56<00:04,  3.87it/s]Processed prompts:  83%|████████▎ | 83/100 [00:56<00:03,  5.37it/s]Processed prompts:  86%|████████▌ | 86/100 [00:56<00:02,  6.70it/s]Processed prompts:  87%|████████▋ | 87/100 [00:56<00:02,  5.40it/s]Processed prompts:  88%|████████▊ | 88/100 [00:56<00:02,  5.53it/s]Processed prompts:  89%|████████▉ | 89/100 [00:57<00:02,  5.45it/s]Processed prompts:  91%|█████████ | 91/100 [00:57<00:01,  4.67it/s]Processed prompts:  93%|█████████▎| 93/100 [00:58<00:02,  3.11it/s]Processed prompts:  94%|█████████▍| 94/100 [00:58<00:01,  3.40it/s]Processed prompts:  95%|█████████▌| 95/100 [00:59<00:01,  3.43it/s]Processed prompts:  96%|█████████▌| 96/100 [00:59<00:01,  2.55it/s]Processed prompts:  97%|█████████▋| 97/100 [01:00<00:00,  3.07it/s]Processed prompts:  98%|█████████▊| 98/100 [01:00<00:00,  2.78it/s]Processed prompts:  99%|█████████▉| 99/100 [01:01<00:00,  2.12it/s]Processed prompts: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]
Traceback (most recent call last):
  File "/data/home/usera411/crosseval/evaluate.py", line 419, in <module>
    main(args)
  File "/data/home/usera411/crosseval/evaluate.py", line 324, in main
    metrics_dicts = calculate_metrics(answers, predictions, args.data_type)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/usera411/crosseval/evaluate.py", line 117, in calculate_metrics
    y_true = translate_score_to_win_list(y_true_list)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/usera411/crosseval/evaluate.py", line 16, in translate_score_to_win_list
    if score_list[i][0] - score_list[i][1] > T:
       ~~~~~~~~~~~~~^^^
TypeError: 'int' object is not subscriptable
